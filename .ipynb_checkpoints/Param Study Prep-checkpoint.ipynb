{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "from pathlib import Path\n",
    "import sobol_seq \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AISI 4130 Steel#^# annealed at 865C@SOLIDWORKS Materials$SOLIDWORKS Materials|AISI 4130 Steel#^# annealed at 865C|94$C:/Program Files/SOLIDWORKS Corp/SOLIDWORKS/lang/english/sldmaterials/SOLIDWORKS Materials.sldmat|AISI 4130 Steel#^# annealed at 865C|94\n",
      " 6061-T6 (SS)@SOLIDWORKS Materials$SOLIDWORKS Materials|6061-T6 (SS)|164$C:/Program Files/SOLIDWORKS Corp/SOLIDWORKS/lang/english/sldmaterials/SOLIDWORKS Materials.sldmat|6061-T6 (SS)|164\n",
      " Ti-6Al-4VSolution treated and aged (SS)@SOLIDWORKS Materials$SOLIDWORKS Materials|Ti-6Al-4VSolution treated and aged (SS)|205$C:/Program Files/SOLIDWORKS Corp/SOLIDWORKS/lang/english/sldmaterials/SOLIDWORKS Materials.sldmat|Ti-6Al-4VSolution treated and aged (SS)|205\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CS Length</th>\n",
       "      <th>BB Drop</th>\n",
       "      <th>Stack</th>\n",
       "      <th>SS E</th>\n",
       "      <th>ST Angle</th>\n",
       "      <th>BB OD</th>\n",
       "      <th>TT OD</th>\n",
       "      <th>HT OD</th>\n",
       "      <th>DT OD</th>\n",
       "      <th>CS OD</th>\n",
       "      <th>...</th>\n",
       "      <th>CSB Offset</th>\n",
       "      <th>SS Z</th>\n",
       "      <th>SS Thickness</th>\n",
       "      <th>CS Thickness</th>\n",
       "      <th>TT Thickness</th>\n",
       "      <th>BB Thickness</th>\n",
       "      <th>HT Thickness</th>\n",
       "      <th>ST Thickness</th>\n",
       "      <th>DT Thickness</th>\n",
       "      <th>DT Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ CS Length,  BB Drop,  Stack,  SS E,  ST Angle,  BB OD,  TT OD,  HT OD,  DT OD,  CS OD,  SS OD,  ST OD,  CS F,  HT LX,  ST UX,  HT UX,  HT Angle,  HT Length,  ST Length,  BB Length,  Dropout Offset,  SSB OD,  CSB OD,  Material,  SSB Offset,  CSB Offset,  SS Z,  SS Thickness,  CS Thickness,  TT Thickness,  BB Thickness,  HT Thickness,  ST Thickness,  DT Thickness,  DT Length]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 35 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template=pd.read_csv(\"Design Study Template_DTL3.csv\", encoding=\"ISO-8859-1\")\n",
    "df=template.copy()\n",
    "# print(df)\n",
    "df.reset_index(inplace=True)  \n",
    "if \"index\" in df.columns:\n",
    "    df.drop([\"index\"], axis=1, inplace=True)\n",
    "# print(df)\n",
    "df.drop(df.index[1], inplace=True)\n",
    "df.drop(df.columns[0:2], axis=1, inplace=True)\n",
    "\n",
    "df.columns = df.iloc[0]\n",
    "# print(df.columns)\n",
    "df.drop(df.index[0], inplace=True)\n",
    "\n",
    "#Define specific materials to insert into design templates\n",
    "steel=df.at[2, \" Material\"]\n",
    "aluminum=df.at[3, \" Material\"]\n",
    "titanium=df.at[4, \" Material\"]\n",
    "\n",
    "print(steel)\n",
    "print(aluminum)\n",
    "print(titanium)\n",
    "df.drop(df.index, axis=0, inplace=True)\n",
    "df.head()\n",
    "# df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def shufflethickness(df):\n",
    "\n",
    "def geom_resample(minval, maxval, perc):\n",
    "    maxval=np.full(np.shape(perc),maxval)\n",
    "    return np.multiply(np.exp(np.multiply(np.log(np.divide(maxval, minval)),perc)), minval)\n",
    "\n",
    "def shufflethickness(df):\n",
    "    scalemin = .5\n",
    "    scalemax = 10\n",
    "    \n",
    "    columns=[\"Wall thickness Bottom Bracket\", \"Wall thickness Top tube\", \"Wall thickness Head tube\", \"Wall thickness Down tube\", \"Wall thickness Chain stay\", \"Wall thickness Seat stay\", \"Wall thickness Seat tube\"]\n",
    "    indices=list(df.index)\n",
    "#     for i in range(len(df.index)):\n",
    "#         if i%10!=0:\n",
    "#             indices.append(df.index[i])\n",
    "    random.shuffle(indices)\n",
    "    len_sobol= len(indices)\n",
    "    sobolvec= sobol_seq.i4_sobol_generate(7, len_sobol)\n",
    "    thicknesses=geom_resample(scalemin, scalemax, sobolvec)\n",
    "    thicknesses=pd.DataFrame(thicknesses, columns=columns, index=indices)\n",
    "    print(thicknesses)\n",
    "    df.update(thicknesses)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Data/microBIKED_reduced.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2031670f12b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbiked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../Data/microBIKED_reduced.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbiked_rethick\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshufflethickness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbiked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# print(biked_rethick)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Data/microBIKED_reduced.csv'"
     ]
    }
   ],
   "source": [
    "biked=pd.read_csv(\"BIKED_reduced.csv\", index_col=0)\n",
    "biked_rethick=shufflethickness(biked)\n",
    "# print(biked_rethick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 6, 11, 17, 20, 23, 30, 31, 57, 68, 109, 110, 122, 158, 177, 188, 189, 195, 199, 234, 240, 248, 253, 289, 290, 325, 329, 332, 336, 345, 353, 359, 372, 390, 391, 398, 403, 410, 429, 432, 442, 453, 455, 503, 505, 516, 525, 528, 542, 553, 579, 667, 671, 713, 749, 771, 844, 858, 918, 921, 947, 987, 990, 1013, 1040, 1042, 1048, 1062, 1065, 1110, 1151, 1154, 1169, 1183, 1196, 1218, 1232, 1233, 1253, 1284, 1299, 1333, 1336, 1344, 1345, 1346, 1347, 1355, 1356, 1366, 1368, 1369, 1377, 1382, 1387, 1407, 1413, 1416, 1434, 1435, 1453, 1464, 1472, 1490, 1496, 1555, 1556, 1669, 1693, 1700, 1736, 1761, 1765, 1771, 1783, 1787, 1789, 1790, 1791, 1793, 1801, 1805, 1810, 1823, 1829, 1863, 1873, 1891, 1908, 1926, 1956, 1957, 1962, 1988, 2016, 2036, 2075, 2110, 2148, 2162, 2192, 2259, 2260, 2268, 2313, 2326, 2405, 2422, 2428, 2435, 2472, 2482, 2518, 2596, 2606, 2641, 2643, 2674, 2675, 2676, 2691, 2756, 2808, 2838, 2839, 2866, 2881, 2883, 2930, 2931, 2939, 2968, 2995, 3018, 3032, 3095, 3102, 3117, 3119, 3125, 3126, 3127, 3142, 3145, 3154, 3161, 3214, 3236, 3316, 3338, 3539, 3576, 3602, 3623, 3646, 3682, 3683, 3752, 3779, 3794, 3823, 4044, 4062, 4063, 4064, 4066, 4087, 4127, 4269, 4326, 4327, 4328, 4333, 4334, 4385, 4402, 4701, 4713, 4774]\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    if col==\" CS Length\":\n",
    "        df[col]=biked[\"CS textfield\"]\n",
    "    if col==\" BB Drop\":\n",
    "        df[col]=biked[\"BB textfield\"]\n",
    "    if col==\" Stack\":\n",
    "        df[col]=biked[\"Stack\"]\n",
    "    if col==\" HT Angle\":\n",
    "        df[col]=biked[\"Head angle\"]\n",
    "    if col==\" HT Length\":\n",
    "        df[col]=biked[\"Head tube length textfield\"]\n",
    "    if col==\" SS E\":\n",
    "        df[col]=biked[\"Seat stay junction0\"]\n",
    "    if col==\" ST Angle\":\n",
    "        df[col]=biked[\"Seat angle\"]\n",
    "    if col==\" DT Length\":\n",
    "        df[col]=biked[\"DT Length\"]\n",
    "    if col==\" ST Length\":\n",
    "        df[col]=biked[\"Seat tube length\"]\n",
    "    if col==\" BB OD\":\n",
    "        df[col]=biked[\"BB diameter\"]\n",
    "    if col==\" TT OD\":\n",
    "        df[col]=biked[\"ttd\"]\n",
    "    if col==\" HT OD\":\n",
    "        df[col]=biked[\"Head tube diameter\"]\n",
    "    if col==\" DT OD\":\n",
    "        df[col]=biked[\"dtd\"]\n",
    "    if col==\" CS OD\":\n",
    "        df[col]=biked[\"csd\"]\n",
    "    if col==\" SS OD\":\n",
    "        df[col]=biked[\"ssd\"]\n",
    "    if col==\" ST OD\":\n",
    "        df[col]=biked[\"Seat tube diameter\"]\n",
    "        \n",
    "    if col==\" BB Thickness\":\n",
    "        df[col]=biked[\"Wall thickness Bottom Bracket\"]\n",
    "    if col==\" TT Thickness\":\n",
    "        df[col]=biked[\"Wall thickness Top tube\"]\n",
    "    if col==\" HT Thickness\":\n",
    "        df[col]=biked[\"Wall thickness Head tube\"]\n",
    "    if col==\" DT Thickness\":\n",
    "        df[col]=biked[\"Wall thickness Down tube\"]\n",
    "    if col==\" CS Thickness\":\n",
    "        df[col]=biked[\"Wall thickness Chain stay\"]\n",
    "    if col==\" SS Thickness\":\n",
    "        df[col]=biked[\"Wall thickness Seat stay\"]\n",
    "    if col==\" ST Thickness\":\n",
    "        df[col]=biked[\"Wall thickness Seat tube\"]\n",
    "        \n",
    "    if col==\" BB Length\":\n",
    "        df[col]=biked[\"BB length\"]\n",
    "    if col==\" CS F\":\n",
    "        df[col]=biked[\"Chain stay position on BB\"]\n",
    "    if col==\" SS Z\":\n",
    "        df[col]=biked[\"SSTopZOFFSET\"]\n",
    "    if col==\" Material\":\n",
    "        df[col]=biked[\"MATERIAL\"]\n",
    "    if col==\" HT UX\":\n",
    "        df[col]=biked[\"Head tube upper extension2\"]\n",
    "    if col==\" ST UX\":\n",
    "        df[col]=biked[\"Seat tube extension2\"]\n",
    "    if col==\" HT LX\":\n",
    "        df[col]=biked[\"Head tube lower extension2\"]\n",
    "    if col==\" SSB Offset\":\n",
    "        df[col]=biked[\"SEATSTAYbrdgshift\"]\n",
    "    if col==\" CSB Offset\":\n",
    "        df[col]=biked[\"CHAINSTAYbrdgshift\"]\n",
    "    if col==\" Dropout Offset\":\n",
    "        df[col]=biked[\"Dropout spacing\"]\n",
    "\n",
    "invalid_bikes=[]\n",
    "for idx in df.index:\n",
    "    \n",
    "  \n",
    "    \n",
    "    invalid=False\n",
    "    if df.at[idx, \" CS Length\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" Stack\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" ST Length\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" HT Length\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" DT Length\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" BB Length\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" HT Angle\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" SS E\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" SS Z\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" SSB Offset\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" CSB Offset\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" ST Angle\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" BB OD\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" TT OD\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" HT OD\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" DT OD\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" CS OD\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" SS OD\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" ST OD\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" HT UX\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" HT LX\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" ST UX\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" ST Angle\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" ST Angle\"]>=180:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" HT Angle\"]>=180:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" HT Angle\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" CS F\"]<=0:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" Dropout Offset\"]<=0:\n",
    "        invalid=True\n",
    "#         print(idx)\n",
    "#         invalid=True\n",
    "#     if (df.at[idx, \" ST Length\"]+df.at[idx, \" ST UX\"])*np.sin(df.at[idx, \" ST Angle\"]/180*np.pi)>=1700:\n",
    "#         print(idx)\n",
    "#         invalid=True\n",
    "    if (df.at[idx, \" CS F\"]>df.at[idx, \" BB Length\"])/2:\n",
    "        invalid=True\n",
    "    if df.at[idx, \" SS OD\"] + df.at[idx, \" ST OD\"] < 2*df.at[idx, \" SS Z\"]:\n",
    "        invalid=True\n",
    "        \n",
    "        \n",
    "    #Set bridge diameter values based on whether bridges are on. If bridges are off, set diameter to 0.\n",
    "    if biked.at[idx, \"SEATSTAYbrdgCheck\"]==1:\n",
    "        df.at[idx, \" SSB OD\"]=biked.at[idx, \"SEATSTAYbrdgdia1\"]\n",
    "        if biked.at[idx, \"SEATSTAYbrdgdia1\"]<0:\n",
    "            invalid=True\n",
    "    else:\n",
    "        df.at[idx, \" SSB OD\"]=0\n",
    "    if biked.at[idx, \"CHAINSTAYbrdgCheck\"]==1:\n",
    "        df.at[idx, \" CSB OD\"]=biked.at[idx, \"CHAINSTAYbrdgdia1\"]\n",
    "        if biked.at[idx, \"CHAINSTAYbrdgdia1\"]<0:\n",
    "            invalid=True\n",
    "    else:\n",
    "        df.at[idx, \" CSB OD\"]=0\n",
    "    \n",
    "    #Set Material names\n",
    "    if df.at[idx, \" Material\"]==\"STEEL\":\n",
    "        df.at[idx, \" Material\"]=steel\n",
    "    elif df.at[idx, \" Material\"]==\"TITANIUM\":\n",
    "        df.at[idx, \" Material\"]=titanium\n",
    "#     if df.at[idx, \" Material\"]==\"ALUMINIUM\":\n",
    "    else:\n",
    "        df.at[idx, \" Material\"]=aluminum\n",
    "\n",
    "        \n",
    "    if invalid==True:\n",
    "        invalid_bikes.append(idx)\n",
    "        \n",
    "\n",
    "\n",
    "#Scale values from mm to meters, excluding any angles and material parameter        \n",
    "columns=list(df.columns)\n",
    "columns.remove(\" Material\")\n",
    "columns.remove(\" HT Angle\")\n",
    "columns.remove(\" ST Angle\")\n",
    "scaleddf=df[columns].divide(1000)\n",
    "scaleddf[\" Material\"]=df[\" Material\"]\n",
    "scaleddf[\" HT Angle\"]=df[\" HT Angle\"]\n",
    "scaleddf[\" ST Angle\"]=df[\" ST Angle\"]\n",
    "scaleddf=scaleddf[df.columns]\n",
    "\n",
    "#Save the dataframe before dropping invalid bikes for future use in validity classification\n",
    "scaleddf.to_csv(\"biked_predrop.csv\")\n",
    "print(invalid_bikes)\n",
    "scaleddf.drop(invalid_bikes, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaleddf.to_csv(\"Processed_structural_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "augdf=pd.read_csv(\"Processed_structural_data_backup.csv\", index_col=0)\n",
    "# augdf=pd.read_csv(\"Processed_structural_data.csv\", index_col=0)\n",
    "# augdf=augdf[1150:1200]\n",
    "augdf=augdf[3400:3600]\n",
    "# augdf=augdf.iloc[[1,20,64]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in augdf.index:\n",
    "    if augdf.at[idx, \" CS F\"]==augdf.at[idx, \" BB Length\"]/2:\n",
    "        augdf.at[idx, \" CS F\"]=augdf.at[idx, \" CS F\"]-.0001\n",
    "        print(\"Adjusting CS F for model \" + str(idx) + \" by 0.0001 since it is half of BB length and would cause model to lock!\")\n",
    "\n",
    "\n",
    "template=pd.read_csv(\"Design Study Template_DTL2.csv\", encoding=\"ISO-8859-1\")\n",
    "# if \"index\" in template.columns:\n",
    "#     template.drop([\"index\"], axis=1, inplace=True)\n",
    "newindex=augdf.columns.insert(0,\"Status\")\n",
    "augdf[\"Status\"]=np.full(len(augdf.index), 1)\n",
    "augdf=augdf[newindex]\n",
    "augdf.index=[\"Bike\" + str(i) for i in augdf.index]\n",
    "# print(template)\n",
    "template.reset_index(inplace=True)  \n",
    "units=template.iloc[1]\n",
    "\n",
    "#Old code to drop \"Level_0\", replace this by dropping entries from top of units until reaching correct length\n",
    "#     units.drop([\"level_0\"], axis=0, inplace=True)\n",
    "\n",
    "while len(units.index)>len(augdf.columns):\n",
    "    units.drop([units.index[0]], axis=0, inplace=True)\n",
    "units.index=augdf.columns\n",
    "units.rename(\"Units\", inplace=True)\n",
    "augdf=augdf.append(units)\n",
    "newindices= augdf.index[:-1].insert(0, augdf.index[-1])\n",
    "augdf=augdf.reindex(newindices)\n",
    "augdf.columns.rename(\"Parameters\", inplace=True)\n",
    "\n",
    "augdf=augdf.T.reset_index().T\n",
    "cols=[len(augdf.index)-2, len(augdf.columns)-1] + [\"\"]*(len(augdf.columns)-2)\n",
    "# print(cols)\n",
    "augdf.columns=cols\n",
    "augdf.columns.rename(\"Design Set\", inplace=True)\n",
    "# print(units)\n",
    "# print(augdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "augdf.to_csv(\"generated_param_study_template.csv\", index_label=\"Design Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
